server:
  base_url:
  api_key:
  request_timeout: 120
job:
  model: 
    provider: vllm
    name: "Qwen/QwQ-32B"
    parameters:
      tensor_parallel_size: 2
  source:
    dataset: "nvidia/OpenCodeReasoning"
    config_name: "split_0"
    split: "split_0[:512]"
    streaming: true
  generation:
    parameters:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 32768
  output:
    mode: "upload_hf"
    hf:
      repo_id: RiddleHe/spider-rollouts-openthoughts-3-qwen32b-opencodereasoning-split-0-512
      token: 
      private: false