server:
  base_url: http://localhost:9000
  api_key: 
  request_timeout: 120
job:
  model: 
    provider: vllm
    name: "Qwen/Qwen3-1.7B"
    parameters:
      tensor_parallel_size: 2
  source:
    dataset: "RiddleHe/OpenCodeReasoning-2-questions-dedup-34k-sample-1024"
    config_name: "default"
    split: "train[:8]"
  generation:
    parameters:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 32768
  output:
    mode: "return"
    format: "jsonl"