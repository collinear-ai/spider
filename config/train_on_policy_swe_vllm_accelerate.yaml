job:
  model:
    provider: tinker
    name: collinear-ai/TEMP_qwen3_14_stage1_icml_26
  source:
    dataset: collinear-ai/SWE-rebench-split
    split: stage2
    shuffle: true
  generation:
    on_policy: true
    max_tool_turns: 50
    parameters:
      temperature: 0.7
      top_p: 0.95
      max_tokens: 65536
      tool_choice: auto
    on_policy_options:

      tool_timeout: 150.0
      # Teacher model - HF name for tokenizer
      teacher: Qwen/Qwen3-Coder-480B-A35B-Instruct
      # Fireworks model ID - set this to use Fireworks API instead of loading locally
      fireworks_model: accounts/keith-7ff28a/deployments/b7sgjwt9

      # Training hyperparameters
      learning_rate: 1e-4
      warmup_ratio: 0.1  # cosine scheduler warmup fraction
      groups_per_batch: 32
      lora_rank: 64
      save_every: 25
      kl_penalty_coef: 1.0
      kl_discount_factor: 0.0
      max_grad_norm: 0.2

      # DeepSpeed configuration (optional - for memory efficiency and multi-GPU scaling)
      use_deepspeed: false  # Set to true to enable DeepSpeed ZeRO
      deepspeed_config_path: "config/deepspeed_config_zer2.json"  # Path to config file (uses ZeRO-2, no CPU offload for speed)
      # Note: The config file settings override the programmatic options below
      deepspeed_zero_stage: 2  # 1, 2, or 3 (2 recommended for LoRA - only used if config_path is None)
      deepspeed_offload_optimizer: false  # Only used if config_path is None
      deepspeed_offload_param: false  # Only used if config_path is None (ZeRO-3 only)

      # === Separated vLLM Inference + Training Architecture ===
      # Enable vLLM-based inference (parallel rollouts, proper tool parsing)
      use_vllm_inference: true

      # GPU allocation - ensure no overlap between vLLM and training GPUs
      # Example configurations:
      #   2 GPU:  vllm_gpu_ids: [0], training_gpu_ids: [1]
      #   4 GPU:  vllm_gpu_ids: [0, 1], training_gpu_ids: [2, 3]  (TP=2 for vLLM)
      #   8 GPU:  vllm_gpu_ids: [0, 1, 2, 3], training_gpu_ids: [4, 5, 6, 7]
      vllm_gpu_ids: [4,5,6,7]
      training_gpu_ids: [0,1,2,3]

      # vLLM server configuration
      vllm_tensor_parallel_size: 4 # Should match len(vllm_gpu_ids) for multi-GPU inference
      vllm_gpu_memory_utilization: 0.7  # Reduced to 70% to save memory
      # vLLM batch size optimizations (for better throughput)
      # KV cache size = max_num_batched_tokens × max_tokens × hidden_size × num_layers × 2 (K+V)
      vllm_max_num_batched_tokens: 4096  # Reduced to default to shrink KV cache
      vllm_max_num_seqs: 128  # Reduced from 256 to shrink KV cache (fewer concurrent sequences)

      # Parallel rollout collection
      rollout_workers: 128  # Number of concurrent HTTP requests to vLLM (increased for better throughput)

      # Weight synchronization between training and inference
      weight_sync_steps: 1  # Sync LoRA weights to vLLM every N training steps

      # PPO-style importance sampling clipping
      importance_sampling_clip: 0.2
    

      # Gradient accumulation (process N trajectories per backward pass, then step)
      # Set to 1 to process one at a time, or None/>=num_trajectories to process all at once
      gradient_accumulation_steps: 4  # Process 4 trajectories per backward pass

      # Training precision (bf16, fp16, or fp8 for H100 GPUs)
      training_precision: bf16

      # Optional: wandb logging
      wandb_project: icml26
      wandb_name: qwen3-14b-stage-opd

      # Optional: stop after N tokens trained
      token_budget: 21516302

  output:
    mode: upload_hf
    hf:
      repo_id: collinear-ai/qwen3-14-stage2-opd
      repo_type: model
