# Simple config file for SWE trajectory generation
# Just edit the values below and run: spider-scaffold openhands --config config/swe/my-swe-config.yaml

scaffold:
  type: openhands                    # Scaffold name: "openhands"
  output_dir: "./trajectories"       # Where to save trajectories
  
  # Dataset configuration
  dataset: "collinear-ai/TEMP_spider_openhands_scaffold_test"     # Your HuggingFace dataset name
  split: "train"                      # Dataset split (train/test/validation)
  
  # Instance filtering (optional)
  max_instances: null                 # null = process all, or set a number like 10
  instance_filter: null               # Optional regex filter for instance IDs
  
  # Agent configuration
  agent_class: "CodeActAgent"        # OpenHands agent name (e.g., "CodeActAgent")
  max_iterations: 50                 # Maximum iterations per task
  
  # LLM configuration (choose one):
  # Option 1: Use model string directly (recommended)
  llm_model: "gpt-4o"                    # Your LLM model name
  llm_api_key: null                      # API key (or set via env var)
  llm_api_key_env: "OPENAI_API_KEY"      # Env var name for API key (preferred)
  llm_base_url: null                     # Base URL (optional, for custom providers)
  
  # Examples:
  # - OpenAI: llm_model="gpt-4o", llm_api_key="sk-...", llm_base_url=null
  # - Anthropic: llm_model="anthropic/claude-3.5-sonnet-20241022", llm_api_key="sk-ant-...", llm_base_url=null
  # - Fireworks: llm_model="accounts/fireworks/models/llama-v3-70b-instruct", llm_api_key="...", llm_base_url="https://api.fireworks.ai/inference/v1"
  # - Together: llm_model="meta-llama/Llama-3-70b-chat-hf", llm_api_key="...", llm_base_url="https://api.together.xyz/v1"
  # - vLLM: llm_model="mistralai/Mistral-7B-Instruct-v0.2", llm_api_key="dummy", llm_base_url="http://localhost:8000/v1"
  
  # Option 2: Use OpenHands config name (if you have OpenHands config.toml)
  # llm_config_name: "llm.eval_gpt4_1106_preview"
  
  # Execution configuration
  num_workers: 1                     # Number of parallel workers
  timeout_seconds: 3600             # Timeout per instance in seconds (1 hour)
  max_retries: 5                    # Maximum retries on failure
  
  # Runtime configuration
  runtime: "docker"                 # Runtime type: "docker"
  platform: "linux/amd64"           # Docker platform
  remote_runtime_resource_factor: 1  # Resource factor (1, 2, 4, or 8)
  
  # Agent capabilities
  enable_browser: false             # Enable browser tools
  enable_llm_editor: false         # Enable LLM editor

