# Simple config file for SWE trajectory generation
# Just edit the values below and run: spider-scaffold openhands --config config/swe/my-swe-config.yaml

scaffold:
  type: openhands                    # Scaffold name: "openhands"
  output_dir: "./trajectories"       # Where to save trajectories
  
  # Dataset configuration
  dataset: "SWE-bench/SWE-smith"      # Official SWE-smith dataset with working Docker images
  split: "train"                      # Dataset split (train/test/validation)
  
  # Instance filtering (optional)
  max_instances: 50  # Process 50 instances (8 workers Ã— ~6 instances each)
  instance_filter: "conan-io__conan.*"  # Filter to conan repo instances               # Optional regex filter for instance IDs
  
  # Agent configuration
  agent_class: "CodeActAgent"        # OpenHands agent name (e.g., "CodeActAgent")
  max_iterations: 50                 # Maximum iterations per task
  
  # LLM configuration (choose one):
  # Option 1: Use model string directly (recommended)
  llm_model: "gpt-4o"                    # Your LLM model name
  llm_api_key: null                      # API key (or set via env var)
  llm_api_key_env: "OPENAI_API_KEY"      # Env var name for API key (preferred)
  llm_base_url: null                     # Base URL (optional, for custom providers)
  
  # Examples:
  # - OpenAI: llm_model="gpt-4o", llm_api_key="sk-...", llm_base_url=null
  # - Anthropic: llm_model="anthropic/claude-3.5-sonnet-20241022", llm_api_key="sk-ant-...", llm_base_url=null
  # - Fireworks: llm_model="accounts/fireworks/models/llama-v3-70b-instruct", llm_api_key="...", llm_base_url="https://api.fireworks.ai/inference/v1"
  # - Together: llm_model="meta-llama/Llama-3-70b-chat-hf", llm_api_key="...", llm_base_url="https://api.together.xyz/v1"
  # - vLLM: llm_model="mistralai/Mistral-7B-Instruct-v0.2", llm_api_key="dummy", llm_base_url="http://localhost:8000/v1"
  
  # Option 2: Use OpenHands config name (if you have OpenHands config.toml)
  # llm_config_name: "llm.eval_gpt4_1106_preview"
  
  # Execution configuration
  num_workers: 2                     # Number of parallel workers (reduced to 2 for stability)
  timeout_seconds: 3600             # Timeout per instance in seconds (1 hour)
  max_retries: 5                    # Maximum retries on failure
  
  # Runtime configuration
  runtime: "docker"                 # Runtime type: "docker"
  platform: "linux/amd64"           # Docker platform
  remote_runtime_resource_factor: 1  # Resource factor (1, 2, 4, or 8)
  
  # Agent capabilities
  enable_browser: false             # Enable browser tools
  enable_llm_editor: false         # Enable LLM editor
  
  # HuggingFace upload configuration (optional)
  hf_repo_id: collinear-ai/TEMP_spider_openhands_integration_test                  # HF repo ID (e.g., "username/swe-trajectories"). Set to enable upload.
  hf_private: true                  # Make repo private (true) or public (false)
  hf_config_name: null              # Config name for dataset (e.g., "gpt4o_conan_50iter"). Auto-generated if null.
  
  # To enable HF upload:
  # 1. Set hf_repo_id: "your-username/your-dataset-name"
  # 2. Set HF_TOKEN environment variable: export HF_TOKEN="hf_..."
  # 3. Trajectories will be uploaded after generation completes
  #
  # Example: hf_repo_id: "myorg/swe-smith-trajectories"
  #          hf_config_name: "gpt4o_conan_50iter"  # or leave null for auto-generation
  
  # Note: Using SWE-smith pre-built images (each task has repo already set up)
  # First run per unique task: 8-10 min to build OpenHands runtime on top of task image
  # Subsequent runs of same task: ~10 seconds (cached runtime)
  # Different tasks will trigger new builds due to different base images

